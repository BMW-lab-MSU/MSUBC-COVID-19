# -*- coding: utf-8 -*-
"""CaseCount Covid only

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x10zPIzS40qbGl_FSoR-H8dbxFspsg4D
"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import random
import math
import copy


def import_csse_data(target,ticker):     #Note: In the future import and shape data can be replaced with a csv file to save 24 seconds.
  #Import Case Count Data from the Csse dataset and filter it.
  csse_df = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv')
  #Import Weather Data (Regional)
  weather_df = pd.read_csv('https://github.com/BMW-lab-MSU/MSUBC-COVID-19/blob/70b7ec485ebf7827d6a5de078cae5c749c29a54f/County_data_folder/weather_data_regional.csv?raw=true')
  #Import Stock Data (Regional PFE)
  stock_df = pd.read_csv('https://github.com/BMW-lab-MSU/MSUBC-COVID-19/blob/199526156bccc821fae1bac8fada4c055c0376d5/County_data_folder/stock_regional.csv?raw=true')
  #Filter to target area (Could be changed to just counties in the future or by area (lat/long))
  df = csse_df[csse_df['Province_State'].isin(target)].reset_index()  
  #Remove any Unassigned or out ofs from the the df.                       
  df = df[~df['Admin2'].str.contains('Unassigned|Out of', na=False)].reset_index()   
  weather_df = weather_df[~weather_df['Unnamed: 0'].str.contains('Unassigned|Out of', na=False)].reset_index() 
  weather_df.set_index(weather_df['Unnamed: 0'].str.replace(', US', ''), inplace=True)
  weather_df.drop(columns=['Unnamed: 0'], inplace=True)  
  stock_df = stock_df[~stock_df['Unnamed: 0'].str.contains('Unassigned|Out of', na=False)].reset_index() 
  stock_df.set_index(stock_df['Unnamed: 0'].str.replace(', US', ''), inplace=True)
  stock_df.drop(columns=['Unnamed: 0'], inplace=True)  
  #Remove Unnecessary Columns and change the index to the combined key - ,us
  df = df.drop(df.columns[:12], axis=1)
  df.set_index(df['Combined_Key'].str.replace(', US', ''), inplace=True)
  df.drop(columns=['Combined_Key'], inplace=True)

  all_imports = [df,stock_df,weather_df]

  return all_imports

def shape_data(csse_data, days_previous, start_date, end_date): #More datasources could be added here i.e (weather, historical search data, scraping, etc.) :: You need to work a bit on condencing this section since it was rushed
  #Filter to use training data from previous study
  csse_data = csse_data.loc[:, start_date:end_date]
  #Copy a dataframe to be the same size and then replace everything with county_array at the ilocs
  csse_data2 = csse_data.copy()
  csse_data2 = csse_data2.drop(csse_data2.columns[:1],axis=1)
  for i in range(len(csse_data2)):
    county_array = np.array(csse_data.iloc[i])
    county_array = [county_array[x] - county_array[x-1] for x in range(len(county_array))]
    county_array = county_array[1:]       #Remove the 0 ites at some point
    county_array = [0 if j < 0 else j for j in county_array]
    csse_data2.iloc[i] = county_array
  csse_data = csse_data2

  titles = []
  #Generate the columne names (Will need to be expanded if new data is added)
  column_names = [f'{number}_days_ago' for number in reversed(range(1,days_previous+1))] + [f'day_{number2}_label' for number2 in range(1,8)]             #Note All of these 8s and 7s are hardcoded in bc we wer doing week long predictions, but this could be easily changed if need be.


  all_names = column_names

  #print(all_names)

  Shaped_df = pd.DataFrame(columns = all_names, index = range(len(csse_data)*(len(csse_data.columns)-(7+days_previous))))

  count = 0
  for county in range(len(csse_data)):
    for days in range(len(csse_data.columns)-(7+days_previous)):
      Shaped_df.iloc[count] = csse_data.iloc[county, 0+days:(7+days_previous)+days]
      count += 1
  #print(Shaped_df)

  #print(Shaped_df)

  global data_min 
  data_min = Shaped_df.min()
  global data_max 
  data_max = Shaped_df.max()
  Shaped_df = (Shaped_df - Shaped_df.min())/ (Shaped_df.max() - Shaped_df.min())   #Normalize This will have to be undone at the end to get an accurate MSE/MAE remember this. :: Also we are predicting total cases not new cases by day so this will also need to be fixed at somepoint.

  return Shaped_df

def train_test(df_length, fold, df):
  #Create a random list of indexs that we will use for training/testing
  testing_size = math.ceil(df_length*(9/10))                          #Split the data for 10 fold cross val
  #Get the size of the testing
  random_list = random.sample(range(df_length), df_length) 
  #Grab this fold's data
  testing = random_list[testing_size*fold:testing_size*(fold+1)]
  training = list(set(random_list).difference(testing))               #This data is in order I don't really thing it matters that it is since we can maintain that order in training. Something to think about.
  this_fold = [df.iloc[testing,:-7], df.iloc[training,:-7], df.loc[testing], df.loc[training]]  #Test,Train,Test_labels,Train_labels
  return this_fold

def initilize_population(weights_list, number_of_pop, max_weight):   #Add in LSTM Layer eventually   :    Idea adjust wieghts as a feture in the network. (Differnt sized things) Don't do everything at once but this is a good idea :: (input might need to be made global idk)
  population = []
  #For the designated population size create i amount of creatures with the initilized weights specified by the weights list.
  for i in range(number_of_pop):
    this_creature = []
    for j in range(len(weights_list)-1):
      weights = [[random.uniform(0,max_weight) for _ in range(weights_list[j+1])] for __ in range(weights_list[j])]
      this_creature.append(weights)
    population.append(this_creature)    
  return population

def activation_function_sigmoid(i,j,values,creature):
  l = []
  for k in range(len(values[i])):
    l.append(float(values[i][k])*float(creature[i][k][j]))  #do xiwi
  sigmoid = 1/(1+(math.e**(-sum(l))))    #sigmoid function
  return sigmoid

def activation_function_linear(values,creature):   #Ouput (Don't think it will be used anywhere esle but if it is the -1 needs to be updated to something dynamic)
  l= []
  for k in range(len(values[-1])):  
    l.append(float(values[-1][k])*float(creature[-1][k][0]))  #do xiwi     #This is just going to predict the same for all seven days. I think that we need to have a bias or something? idk :: Instead we will just do 7 differrnt runs at it I think like "7 folds or something"
  output = sum(l)
  return output

def forward_propegation(input,population): #Make this network adaptable
  values = [input]
  for j in range(len(population)-1): #"i"  #For the number of layers in the network except the last layer :: "i" should be adjustable just in case we change the size of the network.
    layer_outputs = []  
    for k in range(len(population[j+1])):   #"j"
      activation_output = activation_function_sigmoid(j,k,values,population)   
      layer_outputs.append(activation_output) 
    values.append(layer_outputs)
  linear_activation_output = activation_function_linear(values,population)
  values.append(linear_activation_output)
  output = values[-1]
  return output
  
def fitness(guess,actual): #We will do MAE/MSE stuff here
  return np.average([abs(x - y) for x, y in zip(guess, actual)])

def selection(fitness,population):
  new_population = []
  matchup = random.sample(range(len(population)), len(population)) 
  for i in range(int(len(population)/2)):
    new_population.append(population[fitness.index(min(fitness[matchup[i*2]],fitness[matchup[(i*2)+1]]))])
  return new_population

def crossover(population):  #If we could somehow condense this that would be great
  breeding_matchup = random.sample(range(len(population)), len(population))  
  for i in range(int(len(population)/2)):
    parent1 = copy.deepcopy(population[breeding_matchup[i*2]])
    parent2 = copy.deepcopy(population[breeding_matchup[(i*2)+1]])
    cross_layer = random.randint(0,len(parent1)-1)    #Select random layer
    cross_node = random.randint(0,len(parent1[cross_layer])-1)     #select a random starting node
    cross_weight = random.randint(0,len(parent1[cross_layer][cross_node])-1)     #select a random weight
    crossover_line = int(str(cross_layer) + str(cross_node) + str(cross_weight))

    child1 = copy.deepcopy(parent1)
    child2 = copy.deepcopy(parent2)

    for j in range(len(parent1)):  #Layer
      combo_layer = parent1[j]
      for k in range(len(combo_layer)): #Node
        combo_layer2 = parent1[j][k]
        for l in range(len(combo_layer2)): #Weight
          cur_num = int(str(j) + str(k) + str(l))
          if cur_num > crossover_line:
            child1[j][k][l] = parent2[j][k][l]
            child2[j][k][l] = parent1[j][k][l]
    population.append(child1)
    population.append(child2)
  return population
  
def mutation(population, mutation_rate,x):
  for i in range(len(population)):
    cur_child = population[i]
    for j in range(len(cur_child)):
      cur_layer = cur_child[j]
      for k in range(len(cur_layer)):
        cur_node = cur_layer[k]
        for l in range(len(cur_node)):
          random_prob = random.uniform(0,1)
          if random_prob < mutation_rate:
            population[i][j][k][l] = population[i][j][k][l] + random.uniform(-x,x)
  return population

#What states we will use for training
target_states = ["Montana", "Idaho", "Wyoming", "North Dakota", "South Dakota"]
#How many days we will use in training
training_days = 14   
unique_features = 1                      #Current Features: Weather and Case Counts  
start_date = '1/22/21'                   #start_date = '1/22/20'    For now shorter date range   #If we set this 1 day farther back in the code we can fix the code just fyi right now we are removing the first day that we say here in the shaped_df thing I think
end_date = '3/7/21'                      #end_date = '3/7/21'    
data_min = 0
data_max = 0


#Network Parameters
nodes_per_layer = [training_days*unique_features,10,5,3,1]   #Input is training_days output just one day, but we will cycle through which day we are predicting.
population_size = 24
max_weight = 1
number_of_gens = 50
mutation_rate = 0.002
mutation_strength = 4
ticker = "PFE"
#Later
a = 1  #Magnitude
b = 0  #Bias
c = 1  #Shape

all_fitness = []

imported_df = import_csse_data(target_states,ticker)
shaped_df = shape_data(imported_df[0], training_days,start_date,end_date) #Technically Can control the datasets by taking these out?

# :: Here we need to put in a for loop to do every for
for folds in range(10):    #10 fold cross val Testing
  train_test_data = train_test(len(shaped_df),folds,shaped_df) 
  population = initilize_population(nodes_per_layer, population_size, max_weight)
  for days_of_the_week in range(1):
    plot_stuff = []
    for generations in range(number_of_gens):
      all_fitness = []
      for i in range(len(population)):
        training_outputs = []
        for j in range(len(train_test_data[1])):
          training_outputs.append(forward_propegation(train_test_data[1].values[j].astype('float'), population[i]))
        all_fitness.append(fitness(training_outputs, list(train_test_data[3][f'day_{days_of_the_week+1}_label'])))
      print(np.average(all_fitness))
      plot_stuff.append(np.average(all_fitness))
      population = selection(all_fitness,population)    #Something like [new_population]
      population = crossover(population)
      population = mutation(population, mutation_rate, mutation_strength)

    plt.plot(plot_stuff) 
  testing_outputs = []
  for j in range(len(train_test_data[0])):
    testing_outputs.append(forward_propegation(train_test_data[0].values[j].astype('float'), population[all_fitness.index(min(all_fitness))]))
  final_fitness = fitness(testing_outputs, list(train_test_data[2][f'day_{days_of_the_week+1}_label']))
  all_fitness.append((final_fitness*(data_max[training_days*unique_features]-data_min[training_days*unique_features])+data_min[training_days*unique_features]))

  print('Testing Fitness: ',final_fitness)
  print('De-Normalized MAE', (final_fitness*(data_max[training_days*unique_features]-data_min[training_days*unique_features])+data_min[training_days*unique_features]))

print('Average_Fitness: ', np.average(all_fitness))

  #First Record MAE = 3.26 --- #Needs code review
  #                   3.04 Hyper Parameters ([training_days,10,5,3,1], 8, 2, 500, 0.006, 1) 
  #                   3.03 Hyper Parameters ([training_days,10,5,3,1], 24, 1, 500, 0.001, 4)   Nice curve too 
  #                   2.60 Hyper Parameters ([training_days,10,5,3,1], 24, 1, 500, 0.001, 4, 14 days) + Weather
  #                   2.56 Hyper Parameters ([training_days,10,5,3,1], 8, 2, 500, 0.001, 10, 14 days) + Weather + Stock

  #Need 1. More info
  #     2. Change Activation (a,b,and c will have to be put through everything)
  #     3. Change Network size or something
  #     4. Orgainze code and read through comments for more suggestions. (Convert Globals to Returns or something)  https://note.nkmk.me/en/python-function-return-multiple-values/#:~:text=In%20Python%2C%20you%20can%20return,return%20them%20separated%20by%20commas.&text=In%20Python%2C%20comma%2Dseparated%20values,except%20where%20required%20by%20syntax.

shaped_df